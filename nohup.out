  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2021-11-23 03:20:10,424[0m] {[34mscheduler_job.py:[0m1247} INFO[0m - Starting the scheduler[0m
[[34m2021-11-23 03:20:10,424[0m] {[34mscheduler_job.py:[0m1252} INFO[0m - Processing each file at most -1 times[0m
[[34m2021-11-23 03:20:10,427[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 25780[0m
[[34m2021-11-23 03:20:10,428[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 03:20:10,432[0m] {[34msettings.py:[0m54} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2021-11-23 03:20:10,437[0m] {[34mscheduler_job.py:[0m1856} INFO[0m - Marked 2 SchedulerJob instances as failed[0m
[[34m2021-11-23 03:25:10,498[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 03:30:10,549[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 03:35:10,600[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 03:40:10,644[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 03:45:10,685[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 03:50:10,748[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 03:55:10,789[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 04:00:10,827[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 04:05:10,874[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 04:10:10,916[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 04:15:10,955[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 04:20:10,999[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 04:25:11,049[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 04:30:11,092[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 04:35:11,132[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 04:40:11,175[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 04:45:11,215[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 04:50:11,234[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 04:55:11,275[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 05:00:11,329[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 05:05:11,377[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 05:10:11,429[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 05:15:11,475[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 05:20:11,530[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 05:25:11,578[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 05:30:11,623[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 05:35:11,684[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 05:40:11,724[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 05:45:11,762[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 05:50:11,811[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 05:55:11,852[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 06:00:11,892[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 06:05:11,926[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 06:10:11,966[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 06:15:12,015[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 06:20:12,101[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 06:25:12,170[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 06:30:12,218[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 06:35:12,269[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 06:40:12,312[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 06:45:12,357[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 06:50:12,397[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 06:55:12,460[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 07:00:12,521[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 07:05:12,588[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 07:10:12,630[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 07:15:12,676[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 07:20:12,722[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 07:25:12,763[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 07:30:12,808[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 07:35:12,853[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 07:40:12,908[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 07:45:12,946[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 07:50:13,012[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 07:55:13,052[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 08:00:13,096[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 08:05:13,136[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 08:10:13,196[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 08:15:13,251[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 08:20:13,359[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 08:25:13,424[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 08:30:13,468[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 08:35:13,508[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 08:40:13,550[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 08:45:13,618[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 08:50:13,711[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 08:55:13,773[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 09:00:13,821[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 09:05:13,868[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 09:10:13,912[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 09:15:13,951[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 09:20:14,009[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 09:25:14,052[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 09:30:14,092[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 09:35:14,131[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 09:40:14,171[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 09:45:14,232[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 09:50:14,302[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 09:55:14,371[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 10:00:14,413[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 10:05:14,455[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 10:10:14,498[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 10:15:14,560[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 10:20:14,616[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 10:25:14,658[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 10:30:14,696[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 10:35:14,737[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 10:40:14,783[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 10:45:14,825[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 10:50:14,839[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 10:55:14,880[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 11:00:14,921[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 11:05:14,969[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 11:10:15,009[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 11:15:15,104[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 11:20:15,204[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 11:25:15,249[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 11:30:15,297[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 11:35:15,348[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 11:40:15,395[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 11:45:15,435[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 11:50:15,494[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 11:55:15,539[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 12:00:15,601[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 12:05:15,646[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 12:10:15,669[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 12:15:15,714[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 12:20:15,773[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 12:25:15,818[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 12:30:15,885[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 12:35:15,946[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 12:40:15,994[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 12:45:16,052[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 12:50:16,108[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 12:55:16,153[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 13:00:16,222[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 13:05:16,265[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 13:10:16,306[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 13:15:16,353[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 13:20:16,404[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 13:25:16,444[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 13:30:16,485[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 13:35:16,548[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 13:40:16,592[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 13:45:16,636[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 13:50:16,693[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 13:55:16,737[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 14:00:16,779[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 14:05:16,825[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 14:10:16,870[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 14:15:16,911[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 14:20:16,966[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 14:25:17,007[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 14:30:17,050[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 14:35:17,098[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 14:40:17,122[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 14:45:17,160[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 14:50:17,210[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 14:55:17,245[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 15:00:17,307[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 15:05:17,368[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 15:10:17,422[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 15:15:17,475[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 15:20:17,525[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 15:25:17,563[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 15:30:17,604[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 15:35:17,672[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 15:40:17,720[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 15:45:17,760[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 15:50:17,802[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 15:55:17,865[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 16:00:17,912[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 16:05:17,951[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 16:10:18,051[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 16:15:18,094[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 16:20:18,153[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 16:25:18,192[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 16:30:18,243[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 16:35:18,287[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 16:40:18,334[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 16:45:18,395[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 16:50:18,461[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 16:55:18,520[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 17:00:18,574[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 17:05:18,628[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 17:10:18,674[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 17:15:18,736[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 17:20:18,791[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 17:25:18,835[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 17:30:18,888[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 17:35:18,933[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 17:40:19,058[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 17:45:19,100[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 17:50:19,169[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 17:55:19,225[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 18:00:19,267[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 18:05:19,313[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 18:10:19,356[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 18:15:19,404[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 18:20:19,465[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 18:25:19,513[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 18:30:19,557[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 18:35:19,598[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 18:40:19,639[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 18:45:19,684[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 18:50:19,743[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 18:55:19,784[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 19:00:19,804[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 19:05:19,847[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 19:10:19,892[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 19:15:19,931[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 19:20:19,990[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 19:25:20,064[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 19:30:20,128[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 19:35:20,175[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 19:40:20,216[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 19:45:20,255[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 19:50:20,314[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 19:55:20,360[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 20:00:20,427[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 20:05:20,475[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 20:10:20,520[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 20:11:19,692[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 2 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 20:11:18.959155+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 20:11:18.959155+00:00 [scheduled]>[0m
[[34m2021-11-23 20:11:19,703[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued[0m
[[34m2021-11-23 20:11:19,704[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 0/16 running and queued tasks[0m
[[34m2021-11-23 20:11:19,704[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 1/16 running and queued tasks[0m
[[34m2021-11-23 20:11:19,704[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 20:11:18.959155+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 20:11:18.959155+00:00 [scheduled]>[0m
[[34m2021-11-23 20:11:19,707[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Set_Variables', execution_date=datetime.datetime(2021, 11, 23, 20, 11, 18, 959155, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 20:11:19,707[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T20:11:18.959155+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:11:19,707[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='File_Type', execution_date=datetime.datetime(2021, 11, 23, 20, 11, 18, 959155, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 20:11:19,707[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T20:11:18.959155+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:11:19,709[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T20:11:18.959155+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:11:21,031[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 20:11:24,772[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 20:11:24,772[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23T20:11:18.959155+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 20:11:25,693[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T20:11:18.959155+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:11:26,456[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 20:11:27,886[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 20:11:27,887[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23T20:11:18.959155+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 20:11:28,716[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Set_Variables execution_date=2021-11-23 20:11:18.959155+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 20:11:28,717[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.File_Type execution_date=2021-11-23 20:11:18.959155+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 20:11:28,779[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 20:11:18.959155+00:00 [scheduled]>[0m
[[34m2021-11-23 20:11:28,781[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 20:11:28,781[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 0/16 running and queued tasks[0m
[[34m2021-11-23 20:11:28,781[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 20:11:18.959155+00:00 [scheduled]>[0m
[[34m2021-11-23 20:11:28,783[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Print_Variables', execution_date=datetime.datetime(2021, 11, 23, 20, 11, 18, 959155, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-11-23 20:11:28,783[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T20:11:18.959155+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:11:28,784[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T20:11:18.959155+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:11:29,566[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 20:11:31,134[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 20:11:31,135[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23T20:11:18.959155+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 20:11:31,901[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Print_Variables execution_date=2021-11-23 20:11:18.959155+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 20:11:31,962[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 20:11:18.959155+00:00 [scheduled]>[0m
[[34m2021-11-23 20:11:31,964[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 20:11:31,964[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 0/16 running and queued tasks[0m
[[34m2021-11-23 20:11:31,964[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 20:11:18.959155+00:00 [scheduled]>[0m
[[34m2021-11-23 20:11:31,965[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='snowflake_op_sql_str', execution_date=datetime.datetime(2021, 11, 23, 20, 11, 18, 959155, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-11-23 20:11:31,965[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T20:11:18.959155+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:11:31,967[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T20:11:18.959155+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:11:32,728[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 20:11:34,926[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 20:11:34,927[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23T20:11:18.959155+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 20:11:37,195[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.snowflake_op_sql_str execution_date=2021-11-23 20:11:18.959155+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 20:11:37,253[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.sns-msg 2021-11-23 20:11:18.959155+00:00 [scheduled]>[0m
[[34m2021-11-23 20:11:37,255[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 20:11:37,255[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 0/16 running and queued tasks[0m
[[34m2021-11-23 20:11:37,255[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.sns-msg 2021-11-23 20:11:18.959155+00:00 [scheduled]>[0m
[[34m2021-11-23 20:11:37,257[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='sns-msg', execution_date=datetime.datetime(2021, 11, 23, 20, 11, 18, 959155, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-11-23 20:11:37,258[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sns-msg', '2021-11-23T20:11:18.959155+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:11:37,259[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sns-msg', '2021-11-23T20:11:18.959155+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:11:38,051[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 20:11:39,803[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 20:11:39,804[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.sns-msg 2021-11-23T20:11:18.959155+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 20:11:40,685[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.sns-msg execution_date=2021-11-23 20:11:18.959155+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 20:11:40,722[0m] {[34mdagrun.py:[0m430} ERROR[0m - Marking run <DagRun call_process_vendor_inventory @ 2021-11-23 20:11:18.959155+00:00: manual__2021-11-23T20:11:18.959155+00:00, externally triggered: True> failed[0m
[[34m2021-11-23 20:14:15,179[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 2 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 20:14:15.081640+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 20:14:15.081640+00:00 [scheduled]>[0m
[[34m2021-11-23 20:14:15,182[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued[0m
[[34m2021-11-23 20:14:15,182[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 0/16 running and queued tasks[0m
[[34m2021-11-23 20:14:15,182[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 1/16 running and queued tasks[0m
[[34m2021-11-23 20:14:15,182[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 20:14:15.081640+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 20:14:15.081640+00:00 [scheduled]>[0m
[[34m2021-11-23 20:14:15,185[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Set_Variables', execution_date=datetime.datetime(2021, 11, 23, 20, 14, 15, 81640, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 20:14:15,185[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T20:14:15.081640+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:14:15,185[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='File_Type', execution_date=datetime.datetime(2021, 11, 23, 20, 14, 15, 81640, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 20:14:15,185[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T20:14:15.081640+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:14:15,187[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T20:14:15.081640+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:14:16,446[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 20:14:18,365[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 20:14:18,366[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23T20:14:15.081640+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 20:14:19,181[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T20:14:15.081640+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:14:20,058[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 20:14:21,533[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 20:14:21,533[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23T20:14:15.081640+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 20:14:22,322[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Set_Variables execution_date=2021-11-23 20:14:15.081640+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 20:14:22,323[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.File_Type execution_date=2021-11-23 20:14:15.081640+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 20:14:22,396[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 20:14:15.081640+00:00 [scheduled]>[0m
[[34m2021-11-23 20:14:22,398[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 20:14:22,398[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 0/16 running and queued tasks[0m
[[34m2021-11-23 20:14:22,398[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 20:14:15.081640+00:00 [scheduled]>[0m
[[34m2021-11-23 20:14:22,400[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Print_Variables', execution_date=datetime.datetime(2021, 11, 23, 20, 14, 15, 81640, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-11-23 20:14:22,400[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T20:14:15.081640+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:14:22,402[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T20:14:15.081640+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:14:23,261[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 20:14:24,700[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 20:14:24,700[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23T20:14:15.081640+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 20:14:25,531[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Print_Variables execution_date=2021-11-23 20:14:15.081640+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 20:14:25,581[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 20:14:15.081640+00:00 [scheduled]>[0m
[[34m2021-11-23 20:14:25,582[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 20:14:25,583[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 0/16 running and queued tasks[0m
[[34m2021-11-23 20:14:25,583[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 20:14:15.081640+00:00 [scheduled]>[0m
[[34m2021-11-23 20:14:25,584[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='snowflake_op_sql_str', execution_date=datetime.datetime(2021, 11, 23, 20, 14, 15, 81640, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-11-23 20:14:25,584[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T20:14:15.081640+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:14:25,586[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T20:14:15.081640+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:14:26,353[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 20:14:27,800[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 20:14:27,800[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23T20:14:15.081640+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 20:14:29,770[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.snowflake_op_sql_str execution_date=2021-11-23 20:14:15.081640+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 20:14:29,828[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.sns-msg 2021-11-23 20:14:15.081640+00:00 [scheduled]>[0m
[[34m2021-11-23 20:14:29,829[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 20:14:29,829[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 0/16 running and queued tasks[0m
[[34m2021-11-23 20:14:29,829[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.sns-msg 2021-11-23 20:14:15.081640+00:00 [scheduled]>[0m
[[34m2021-11-23 20:14:29,831[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='sns-msg', execution_date=datetime.datetime(2021, 11, 23, 20, 14, 15, 81640, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-11-23 20:14:29,831[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sns-msg', '2021-11-23T20:14:15.081640+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:14:29,832[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sns-msg', '2021-11-23T20:14:15.081640+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:14:30,618[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 20:14:32,142[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 20:14:32,142[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.sns-msg 2021-11-23T20:14:15.081640+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 20:14:32,859[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.sns-msg execution_date=2021-11-23 20:14:15.081640+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 20:14:32,897[0m] {[34mdagrun.py:[0m430} ERROR[0m - Marking run <DagRun call_process_vendor_inventory @ 2021-11-23 20:14:15.081640+00:00: manual__2021-11-23T20:14:15.081640+00:00, externally triggered: True> failed[0m
[[34m2021-11-23 20:15:20,566[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 20:20:18,053[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 2 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 20:20:17.988500+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 20:20:17.988500+00:00 [scheduled]>[0m
[[34m2021-11-23 20:20:18,055[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued[0m
[[34m2021-11-23 20:20:18,055[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 0/16 running and queued tasks[0m
[[34m2021-11-23 20:20:18,055[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 1/16 running and queued tasks[0m
[[34m2021-11-23 20:20:18,055[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 20:20:17.988500+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 20:20:17.988500+00:00 [scheduled]>[0m
[[34m2021-11-23 20:20:18,057[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Set_Variables', execution_date=datetime.datetime(2021, 11, 23, 20, 20, 17, 988500, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 20:20:18,057[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T20:20:17.988500+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:20:18,057[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='File_Type', execution_date=datetime.datetime(2021, 11, 23, 20, 20, 17, 988500, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 20:20:18,057[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T20:20:17.988500+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:20:18,058[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T20:20:17.988500+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:20:18,970[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 20:20:20,516[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 20:20:20,517[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23T20:20:17.988500+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 20:20:21,385[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T20:20:17.988500+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:20:22,153[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 20:20:23,823[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 20:20:23,823[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23T20:20:17.988500+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 20:20:24,837[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Set_Variables execution_date=2021-11-23 20:20:17.988500+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 20:20:24,838[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.File_Type execution_date=2021-11-23 20:20:17.988500+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 20:20:24,878[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 20:20:24,886[0m] {[34mscheduler_job.py:[0m1901} INFO[0m - Reset the following 1 orphaned TaskInstances:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 20:20:17.988500+00:00 [scheduled]>[0m
[[34m2021-11-23 20:20:24,934[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 20:20:17.988500+00:00 [scheduled]>[0m
[[34m2021-11-23 20:20:24,936[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 20:20:24,936[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 0/16 running and queued tasks[0m
[[34m2021-11-23 20:20:24,936[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 20:20:17.988500+00:00 [scheduled]>[0m
[[34m2021-11-23 20:20:24,938[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Print_Variables', execution_date=datetime.datetime(2021, 11, 23, 20, 20, 17, 988500, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-11-23 20:20:24,938[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T20:20:17.988500+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:20:24,939[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T20:20:17.988500+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:20:25,711[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 20:20:27,262[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 20:20:27,262[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23T20:20:17.988500+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 20:20:28,026[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Print_Variables execution_date=2021-11-23 20:20:17.988500+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 20:20:28,077[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 20:20:17.988500+00:00 [scheduled]>[0m
[[34m2021-11-23 20:20:28,078[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 20:20:28,079[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 0/16 running and queued tasks[0m
[[34m2021-11-23 20:20:28,079[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 20:20:17.988500+00:00 [scheduled]>[0m
[[34m2021-11-23 20:20:28,080[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='snowflake_op_sql_str', execution_date=datetime.datetime(2021, 11, 23, 20, 20, 17, 988500, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-11-23 20:20:28,080[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T20:20:17.988500+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:20:28,082[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T20:20:17.988500+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:20:28,851[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 20:20:30,379[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 20:20:30,380[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23T20:20:17.988500+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 20:20:33,264[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.snowflake_op_sql_str execution_date=2021-11-23 20:20:17.988500+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 20:20:33,374[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.sns-msg 2021-11-23 20:20:17.988500+00:00 [scheduled]>[0m
[[34m2021-11-23 20:20:33,376[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 20:20:33,376[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 0/16 running and queued tasks[0m
[[34m2021-11-23 20:20:33,376[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.sns-msg 2021-11-23 20:20:17.988500+00:00 [scheduled]>[0m
[[34m2021-11-23 20:20:33,378[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='sns-msg', execution_date=datetime.datetime(2021, 11, 23, 20, 20, 17, 988500, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-11-23 20:20:33,378[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sns-msg', '2021-11-23T20:20:17.988500+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:20:33,380[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sns-msg', '2021-11-23T20:20:17.988500+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 20:20:34,175[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 20:20:36,857[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 20:20:36,859[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.sns-msg 2021-11-23T20:20:17.988500+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 20:20:37,966[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.sns-msg execution_date=2021-11-23 20:20:17.988500+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 20:20:38,015[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun call_process_vendor_inventory @ 2021-11-23 20:20:17.988500+00:00: manual__2021-11-23T20:20:17.988500+00:00, externally triggered: True> successful[0m
[[34m2021-11-23 20:25:24,922[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 20:30:24,968[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 20:35:24,991[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 20:40:25,046[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 20:45:25,110[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 20:50:25,168[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 20:55:25,209[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 21:00:25,265[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 21:05:25,308[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 21:10:25,349[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 21:15:25,392[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 21:20:25,443[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 21:21:09,233[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 2 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 21:21:08.463247+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 21:21:08.463247+00:00 [scheduled]>[0m
[[34m2021-11-23 21:21:09,236[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued[0m
[[34m2021-11-23 21:21:09,236[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 0/16 running and queued tasks[0m
[[34m2021-11-23 21:21:09,236[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 1/16 running and queued tasks[0m
[[34m2021-11-23 21:21:09,236[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 21:21:08.463247+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 21:21:08.463247+00:00 [scheduled]>[0m
[[34m2021-11-23 21:21:09,247[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Set_Variables', execution_date=datetime.datetime(2021, 11, 23, 21, 21, 8, 463247, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 21:21:09,247[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T21:21:08.463247+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:21:09,247[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='File_Type', execution_date=datetime.datetime(2021, 11, 23, 21, 21, 8, 463247, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 21:21:09,247[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T21:21:08.463247+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:21:09,249[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T21:21:08.463247+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:21:10,922[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:21:12,824[0m] {[34mdagbag.py:[0m305} ERROR[0m - Failed to import: /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
Traceback (most recent call last):
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/models/dagbag.py", line 302, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/airflow/airflow/dags/_call_process_vendor_inventory.py", line 126, in <module>
    set_vars_task >> print_vars_task >>  snowflake_op_sql_str >> file_loaded >> test_sns
NameError: name 'test_sns' is not defined
[[34m2021-11-23 21:21:12,847[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:21:12,848[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Traceback (most recent call last):
  File "/home/airflow/.venv/airflow/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/__main__.py", line 40, in main
    args.func(args)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/cli.py", line 89, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/cli/commands/task_command.py", line 217, in task_run
    dag = get_dag(args.subdir, args.dag_id)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/cli.py", line 189, in get_dag
    'parse.'.format(dag_id)
airflow.exceptions.AirflowException: dag_id could not be found: call_process_vendor_inventory. Either the dag did not exist or it failed to parse.
[[34m2021-11-23 21:21:13,194[0m] {[34msequential_executor.py:[0m66} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T21:21:08.463247+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py']' returned non-zero exit status 1..[0m
[[34m2021-11-23 21:21:13,195[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T21:21:08.463247+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:21:13,977[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:21:15,666[0m] {[34mdagbag.py:[0m305} ERROR[0m - Failed to import: /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
Traceback (most recent call last):
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/models/dagbag.py", line 302, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/airflow/airflow/dags/_call_process_vendor_inventory.py", line 126, in <module>
    set_vars_task >> print_vars_task >>  snowflake_op_sql_str >> file_loaded >> test_sns
NameError: name 'test_sns' is not defined
[[34m2021-11-23 21:21:15,689[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:21:15,689[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Traceback (most recent call last):
  File "/home/airflow/.venv/airflow/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/__main__.py", line 40, in main
    args.func(args)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/cli.py", line 89, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/cli/commands/task_command.py", line 217, in task_run
    dag = get_dag(args.subdir, args.dag_id)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/cli.py", line 189, in get_dag
    'parse.'.format(dag_id)
airflow.exceptions.AirflowException: dag_id could not be found: call_process_vendor_inventory. Either the dag did not exist or it failed to parse.
[[34m2021-11-23 21:21:16,040[0m] {[34msequential_executor.py:[0m66} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T21:21:08.463247+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py']' returned non-zero exit status 1..[0m
[[34m2021-11-23 21:21:16,041[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Set_Variables execution_date=2021-11-23 21:21:08.463247+00:00 exited with status failed for try_number 1[0m
[[34m2021-11-23 21:21:16,041[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.File_Type execution_date=2021-11-23 21:21:08.463247+00:00 exited with status failed for try_number 1[0m
[[34m2021-11-23 21:21:16,048[0m] {[34mscheduler_job.py:[0m1235} ERROR[0m - Executor reports task instance <TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 21:21:08.463247+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-11-23 21:21:16,048[0m] {[34mscheduler_job.py:[0m1235} ERROR[0m - Executor reports task instance <TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 21:21:08.463247+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-11-23 21:25:25,498[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 21:30:25,566[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 21:35:25,617[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 21:40:25,670[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 21:41:02,697[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 2 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 21:41:02.185629+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 21:41:02.185629+00:00 [scheduled]>[0m
[[34m2021-11-23 21:41:02,717[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 2 task instances ready to be queued[0m
[[34m2021-11-23 21:41:02,717[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 21:41:02,717[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 3/16 running and queued tasks[0m
[[34m2021-11-23 21:41:02,718[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 21:41:02.185629+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 21:41:02.185629+00:00 [scheduled]>[0m
[[34m2021-11-23 21:41:02,727[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Set_Variables', execution_date=datetime.datetime(2021, 11, 23, 21, 41, 2, 185629, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 21:41:02,728[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T21:41:02.185629+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:41:02,728[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='File_Type', execution_date=datetime.datetime(2021, 11, 23, 21, 41, 2, 185629, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 21:41:02,728[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T21:41:02.185629+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:41:02,746[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T21:41:02.185629+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:41:04,371[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:41:06,711[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:41:06,711[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23T21:41:02.185629+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 21:41:07,628[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T21:41:02.185629+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:41:08,428[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:41:09,976[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:41:09,976[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23T21:41:02.185629+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 21:41:10,828[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Set_Variables execution_date=2021-11-23 21:41:02.185629+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 21:41:10,829[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.File_Type execution_date=2021-11-23 21:41:02.185629+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 21:41:10,912[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 21:41:02.185629+00:00 [scheduled]>[0m
[[34m2021-11-23 21:41:10,914[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 21:41:10,914[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 21:41:10,914[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 21:41:02.185629+00:00 [scheduled]>[0m
[[34m2021-11-23 21:41:10,916[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Print_Variables', execution_date=datetime.datetime(2021, 11, 23, 21, 41, 2, 185629, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-11-23 21:41:10,916[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T21:41:02.185629+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:41:10,918[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T21:41:02.185629+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:41:11,688[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:41:13,230[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:41:13,230[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23T21:41:02.185629+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 21:41:13,988[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Print_Variables execution_date=2021-11-23 21:41:02.185629+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 21:41:14,055[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 21:41:02.185629+00:00 [scheduled]>[0m
[[34m2021-11-23 21:41:14,058[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 21:41:14,058[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 21:41:14,058[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 21:41:02.185629+00:00 [scheduled]>[0m
[[34m2021-11-23 21:41:14,060[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='snowflake_op_sql_str', execution_date=datetime.datetime(2021, 11, 23, 21, 41, 2, 185629, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-11-23 21:41:14,061[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T21:41:02.185629+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:41:14,062[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T21:41:02.185629+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:41:14,921[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:41:16,395[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:41:16,395[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23T21:41:02.185629+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 21:41:18,631[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.snowflake_op_sql_str execution_date=2021-11-23 21:41:02.185629+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 21:41:18,720[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.sns-msg 2021-11-23 21:41:02.185629+00:00 [scheduled]>[0m
[[34m2021-11-23 21:41:18,722[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 21:41:18,722[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 21:41:18,723[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.sns-msg 2021-11-23 21:41:02.185629+00:00 [scheduled]>[0m
[[34m2021-11-23 21:41:18,725[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='sns-msg', execution_date=datetime.datetime(2021, 11, 23, 21, 41, 2, 185629, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-11-23 21:41:18,725[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sns-msg', '2021-11-23T21:41:02.185629+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:41:18,727[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sns-msg', '2021-11-23T21:41:02.185629+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:41:19,912[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:41:21,779[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:41:21,780[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.sns-msg 2021-11-23T21:41:02.185629+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 21:41:22,559[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.sns-msg execution_date=2021-11-23 21:41:02.185629+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 21:41:22,597[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun call_process_vendor_inventory @ 2021-11-23 21:41:02.185629+00:00: manual__2021-11-23T21:41:02.185629+00:00, externally triggered: True> successful[0m
[[34m2021-11-23 21:45:25,750[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 21:50:25,810[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 21:54:05,676[0m] {[34mdagrun.py:[0m645} WARNING[0m - Failed to get task '<TaskInstance: call_process_vendor_inventory.sns-msg 2021-11-23 21:21:08.463247+00:00 [None]>' for dag '<DAG: call_process_vendor_inventory>'. Marking it as removed.[0m
[[34m2021-11-23 21:55:25,877[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 21:56:19,848[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 2 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 21:56:19.687080+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 21:56:19.687080+00:00 [scheduled]>[0m
[[34m2021-11-23 21:56:19,850[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 2 task instances ready to be queued[0m
[[34m2021-11-23 21:56:19,850[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 21:56:19,850[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 3/16 running and queued tasks[0m
[[34m2021-11-23 21:56:19,851[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 21:56:19.687080+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 21:56:19.687080+00:00 [scheduled]>[0m
[[34m2021-11-23 21:56:19,855[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Set_Variables', execution_date=datetime.datetime(2021, 11, 23, 21, 56, 19, 687080, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 21:56:19,855[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T21:56:19.687080+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:56:19,856[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='File_Type', execution_date=datetime.datetime(2021, 11, 23, 21, 56, 19, 687080, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 21:56:19,856[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T21:56:19.687080+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:56:19,857[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T21:56:19.687080+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:56:20,737[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:56:22,199[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:56:22,199[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23T21:56:19.687080+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 21:56:23,323[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T21:56:19.687080+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:56:24,441[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:56:25,895[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:56:25,896[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23T21:56:19.687080+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 21:56:26,770[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Set_Variables execution_date=2021-11-23 21:56:19.687080+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 21:56:26,770[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.File_Type execution_date=2021-11-23 21:56:19.687080+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 21:56:26,840[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 21:56:19.687080+00:00 [scheduled]>[0m
[[34m2021-11-23 21:56:26,841[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 21:56:26,842[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 21:56:26,842[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 21:56:19.687080+00:00 [scheduled]>[0m
[[34m2021-11-23 21:56:26,843[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Print_Variables', execution_date=datetime.datetime(2021, 11, 23, 21, 56, 19, 687080, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-11-23 21:56:26,843[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T21:56:19.687080+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:56:26,845[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T21:56:19.687080+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:56:27,618[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:56:29,099[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:56:29,099[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23T21:56:19.687080+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 21:56:29,880[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Print_Variables execution_date=2021-11-23 21:56:19.687080+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 21:56:29,939[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 21:56:19.687080+00:00 [scheduled]>[0m
[[34m2021-11-23 21:56:29,940[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 21:56:29,940[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 21:56:29,941[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 21:56:19.687080+00:00 [scheduled]>[0m
[[34m2021-11-23 21:56:29,942[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='snowflake_op_sql_str', execution_date=datetime.datetime(2021, 11, 23, 21, 56, 19, 687080, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-11-23 21:56:29,942[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T21:56:19.687080+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:56:29,944[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T21:56:19.687080+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:56:30,811[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:56:32,265[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:56:32,266[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23T21:56:19.687080+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 21:56:34,504[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.snowflake_op_sql_str execution_date=2021-11-23 21:56:19.687080+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 21:56:34,582[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.sns_msg 2021-11-23 21:56:19.687080+00:00 [scheduled]>[0m
[[34m2021-11-23 21:56:34,584[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 21:56:34,584[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 21:56:34,584[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.sns_msg 2021-11-23 21:56:19.687080+00:00 [scheduled]>[0m
[[34m2021-11-23 21:56:34,586[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='sns_msg', execution_date=datetime.datetime(2021, 11, 23, 21, 56, 19, 687080, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-11-23 21:56:34,586[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sns_msg', '2021-11-23T21:56:19.687080+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:56:34,588[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sns_msg', '2021-11-23T21:56:19.687080+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:56:35,430[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:56:36,885[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:56:36,886[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.sns_msg 2021-11-23T21:56:19.687080+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 21:56:37,567[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.sns_msg execution_date=2021-11-23 21:56:19.687080+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 21:56:37,605[0m] {[34mdagrun.py:[0m430} ERROR[0m - Marking run <DagRun call_process_vendor_inventory @ 2021-11-23 21:56:19.687080+00:00: manual__2021-11-23T21:56:19.687080+00:00, externally triggered: True> failed[0m
[[34m2021-11-23 21:58:46,428[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 2 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 21:58:46.218558+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 21:58:46.218558+00:00 [scheduled]>[0m
[[34m2021-11-23 21:58:46,429[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 2 task instances ready to be queued[0m
[[34m2021-11-23 21:58:46,429[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 21:58:46,429[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 3/16 running and queued tasks[0m
[[34m2021-11-23 21:58:46,430[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 21:58:46.218558+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 21:58:46.218558+00:00 [scheduled]>[0m
[[34m2021-11-23 21:58:46,431[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Set_Variables', execution_date=datetime.datetime(2021, 11, 23, 21, 58, 46, 218558, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 21:58:46,431[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T21:58:46.218558+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:58:46,432[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='File_Type', execution_date=datetime.datetime(2021, 11, 23, 21, 58, 46, 218558, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 21:58:46,432[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T21:58:46.218558+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:58:46,433[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T21:58:46.218558+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:58:47,236[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:58:48,801[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:58:48,801[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23T21:58:46.218558+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 21:58:49,639[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T21:58:46.218558+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:58:50,410[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:58:52,204[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:58:52,204[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23T21:58:46.218558+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 21:58:53,385[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Set_Variables execution_date=2021-11-23 21:58:46.218558+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 21:58:53,385[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.File_Type execution_date=2021-11-23 21:58:46.218558+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 21:58:53,518[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 21:58:46.218558+00:00 [scheduled]>[0m
[[34m2021-11-23 21:58:53,521[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 21:58:53,521[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 21:58:53,521[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 21:58:46.218558+00:00 [scheduled]>[0m
[[34m2021-11-23 21:58:53,523[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Print_Variables', execution_date=datetime.datetime(2021, 11, 23, 21, 58, 46, 218558, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-11-23 21:58:53,524[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T21:58:46.218558+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:58:53,525[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T21:58:46.218558+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:58:54,449[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:58:55,987[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:58:55,987[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23T21:58:46.218558+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 21:58:56,758[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Print_Variables execution_date=2021-11-23 21:58:46.218558+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 21:58:56,817[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 21:58:46.218558+00:00 [scheduled]>[0m
[[34m2021-11-23 21:58:56,818[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 21:58:56,818[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 21:58:56,819[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 21:58:46.218558+00:00 [scheduled]>[0m
[[34m2021-11-23 21:58:56,820[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='snowflake_op_sql_str', execution_date=datetime.datetime(2021, 11, 23, 21, 58, 46, 218558, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-11-23 21:58:56,820[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T21:58:46.218558+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:58:56,822[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T21:58:46.218558+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:58:57,588[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:59:00,218[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:59:00,219[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23T21:58:46.218558+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 21:59:03,766[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.snowflake_op_sql_str execution_date=2021-11-23 21:58:46.218558+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 21:59:03,852[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.sns_msg 2021-11-23 21:58:46.218558+00:00 [scheduled]>[0m
[[34m2021-11-23 21:59:03,854[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 21:59:03,855[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 21:59:03,855[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.sns_msg 2021-11-23 21:58:46.218558+00:00 [scheduled]>[0m
[[34m2021-11-23 21:59:03,857[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='sns_msg', execution_date=datetime.datetime(2021, 11, 23, 21, 58, 46, 218558, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-11-23 21:59:03,857[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sns_msg', '2021-11-23T21:58:46.218558+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:59:03,859[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sns_msg', '2021-11-23T21:58:46.218558+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 21:59:05,161[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 21:59:07,138[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 21:59:07,138[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.sns_msg 2021-11-23T21:58:46.218558+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 21:59:08,000[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.sns_msg execution_date=2021-11-23 21:58:46.218558+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 21:59:08,043[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun call_process_vendor_inventory @ 2021-11-23 21:58:46.218558+00:00: manual__2021-11-23T21:58:46.218558+00:00, externally triggered: True> successful[0m
[[34m2021-11-23 22:00:25,931[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 22:02:52,116[0m] {[34mdagrun.py:[0m645} WARNING[0m - Failed to get task '<TaskInstance: call_process_vendor_inventory.sns_msg 2021-11-23 21:21:08.463247+00:00 [None]>' for dag '<DAG: call_process_vendor_inventory>'. Marking it as removed.[0m
[[34m2021-11-23 22:05:25,987[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 22:10:26,040[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 22:13:38,214[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 2 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 22:13:38.157236+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 22:13:38.157236+00:00 [scheduled]>[0m
[[34m2021-11-23 22:13:38,215[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 2 task instances ready to be queued[0m
[[34m2021-11-23 22:13:38,215[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:13:38,215[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 3/16 running and queued tasks[0m
[[34m2021-11-23 22:13:38,216[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 22:13:38.157236+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 22:13:38.157236+00:00 [scheduled]>[0m
[[34m2021-11-23 22:13:38,217[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Set_Variables', execution_date=datetime.datetime(2021, 11, 23, 22, 13, 38, 157236, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 22:13:38,217[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T22:13:38.157236+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:13:38,217[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='File_Type', execution_date=datetime.datetime(2021, 11, 23, 22, 13, 38, 157236, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 22:13:38,217[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T22:13:38.157236+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:13:38,219[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T22:13:38.157236+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:13:39,035[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:13:40,478[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:13:40,479[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23T22:13:38.157236+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:13:41,321[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T22:13:38.157236+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:13:42,260[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:13:43,745[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:13:43,746[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23T22:13:38.157236+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:13:44,552[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Set_Variables execution_date=2021-11-23 22:13:38.157236+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:13:44,552[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.File_Type execution_date=2021-11-23 22:13:38.157236+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:13:44,634[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 22:13:38.157236+00:00 [scheduled]>[0m
[[34m2021-11-23 22:13:44,636[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 22:13:44,636[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:13:44,636[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 22:13:38.157236+00:00 [scheduled]>[0m
[[34m2021-11-23 22:13:44,637[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Print_Variables', execution_date=datetime.datetime(2021, 11, 23, 22, 13, 38, 157236, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-11-23 22:13:44,638[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T22:13:38.157236+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:13:44,639[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T22:13:38.157236+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:13:45,411[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:13:46,837[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:13:46,837[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23T22:13:38.157236+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:13:47,647[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Print_Variables execution_date=2021-11-23 22:13:38.157236+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:13:47,703[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 22:13:38.157236+00:00 [scheduled]>[0m
[[34m2021-11-23 22:13:47,705[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 22:13:47,705[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:13:47,705[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 22:13:38.157236+00:00 [scheduled]>[0m
[[34m2021-11-23 22:13:47,707[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='snowflake_op_sql_str', execution_date=datetime.datetime(2021, 11, 23, 22, 13, 38, 157236, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-11-23 22:13:47,707[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T22:13:38.157236+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:13:47,709[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T22:13:38.157236+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:13:48,469[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:13:49,927[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:13:49,927[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23T22:13:38.157236+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:13:52,685[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.snowflake_op_sql_str execution_date=2021-11-23 22:13:38.157236+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:13:52,760[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.sample_file_test 2021-11-23 22:13:38.157236+00:00 [scheduled]>[0m
[[34m2021-11-23 22:13:52,761[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 22:13:52,761[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:13:52,761[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.sample_file_test 2021-11-23 22:13:38.157236+00:00 [scheduled]>[0m
[[34m2021-11-23 22:13:52,763[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='sample_file_test', execution_date=datetime.datetime(2021, 11, 23, 22, 13, 38, 157236, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-11-23 22:13:52,763[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sample_file_test', '2021-11-23T22:13:38.157236+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:13:52,765[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sample_file_test', '2021-11-23T22:13:38.157236+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:13:54,119[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:13:57,291[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:13:57,292[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.sample_file_test 2021-11-23T22:13:38.157236+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:13:58,740[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.sample_file_test execution_date=2021-11-23 22:13:38.157236+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:13:58,836[0m] {[34mdagrun.py:[0m430} ERROR[0m - Marking run <DagRun call_process_vendor_inventory @ 2021-11-23 22:13:38.157236+00:00: manual__2021-11-23T22:13:38.157236+00:00, externally triggered: True> failed[0m
[[34m2021-11-23 22:15:26,094[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 22:20:26,163[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 22:25:26,246[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 22:26:10,142[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 2 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 22:26:09.418942+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 22:26:09.418942+00:00 [scheduled]>[0m
[[34m2021-11-23 22:26:10,143[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 2 task instances ready to be queued[0m
[[34m2021-11-23 22:26:10,144[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:26:10,144[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 3/16 running and queued tasks[0m
[[34m2021-11-23 22:26:10,144[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 22:26:09.418942+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 22:26:09.418942+00:00 [scheduled]>[0m
[[34m2021-11-23 22:26:10,145[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Set_Variables', execution_date=datetime.datetime(2021, 11, 23, 22, 26, 9, 418942, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 22:26:10,146[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T22:26:09.418942+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:26:10,146[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='File_Type', execution_date=datetime.datetime(2021, 11, 23, 22, 26, 9, 418942, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 22:26:10,146[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T22:26:09.418942+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:26:10,147[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T22:26:09.418942+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:26:10,896[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:26:12,335[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:26:12,336[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23T22:26:09.418942+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:26:13,214[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T22:26:09.418942+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:26:13,974[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:26:15,404[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:26:15,405[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23T22:26:09.418942+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:26:16,339[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Set_Variables execution_date=2021-11-23 22:26:09.418942+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:26:16,340[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.File_Type execution_date=2021-11-23 22:26:09.418942+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:26:16,410[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 22:26:09.418942+00:00 [scheduled]>[0m
[[34m2021-11-23 22:26:16,412[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 22:26:16,412[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:26:16,412[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 22:26:09.418942+00:00 [scheduled]>[0m
[[34m2021-11-23 22:26:16,413[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Print_Variables', execution_date=datetime.datetime(2021, 11, 23, 22, 26, 9, 418942, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-11-23 22:26:16,414[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T22:26:09.418942+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:26:16,415[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T22:26:09.418942+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:26:17,166[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:26:18,642[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:26:18,642[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23T22:26:09.418942+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:26:19,406[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Print_Variables execution_date=2021-11-23 22:26:09.418942+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:26:19,490[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 22:26:09.418942+00:00 [scheduled]>[0m
[[34m2021-11-23 22:26:19,492[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 22:26:19,492[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:26:19,493[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 22:26:09.418942+00:00 [scheduled]>[0m
[[34m2021-11-23 22:26:19,495[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='snowflake_op_sql_str', execution_date=datetime.datetime(2021, 11, 23, 22, 26, 9, 418942, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-11-23 22:26:19,495[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T22:26:09.418942+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:26:19,496[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T22:26:09.418942+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:26:20,294[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:26:21,765[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:26:21,766[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23T22:26:09.418942+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:26:24,432[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.snowflake_op_sql_str execution_date=2021-11-23 22:26:09.418942+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:26:24,498[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.sample_file_test 2021-11-23 22:26:09.418942+00:00 [scheduled]>[0m
[[34m2021-11-23 22:26:24,500[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 22:26:24,500[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:26:24,500[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.sample_file_test 2021-11-23 22:26:09.418942+00:00 [scheduled]>[0m
[[34m2021-11-23 22:26:24,502[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='sample_file_test', execution_date=datetime.datetime(2021, 11, 23, 22, 26, 9, 418942, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-11-23 22:26:24,502[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sample_file_test', '2021-11-23T22:26:09.418942+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:26:24,503[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sample_file_test', '2021-11-23T22:26:09.418942+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:26:25,844[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:26:29,101[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:26:29,101[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.sample_file_test 2021-11-23T22:26:09.418942+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:26:31,887[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.sample_file_test execution_date=2021-11-23 22:26:09.418942+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:26:31,968[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun call_process_vendor_inventory @ 2021-11-23 22:26:09.418942+00:00: manual__2021-11-23T22:26:09.418942+00:00, externally triggered: True> successful[0m
[[34m2021-11-23 22:30:26,298[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 22:35:26,367[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 22:40:26,421[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 22:40:38,408[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 2 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 22:40:37.809867+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 22:40:37.809867+00:00 [scheduled]>[0m
[[34m2021-11-23 22:40:38,410[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 2 task instances ready to be queued[0m
[[34m2021-11-23 22:40:38,410[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:40:38,410[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 3/16 running and queued tasks[0m
[[34m2021-11-23 22:40:38,411[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 22:40:37.809867+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 22:40:37.809867+00:00 [scheduled]>[0m
[[34m2021-11-23 22:40:38,412[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Set_Variables', execution_date=datetime.datetime(2021, 11, 23, 22, 40, 37, 809867, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 22:40:38,412[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T22:40:37.809867+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:40:38,413[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='File_Type', execution_date=datetime.datetime(2021, 11, 23, 22, 40, 37, 809867, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 22:40:38,413[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T22:40:37.809867+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:40:38,414[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T22:40:37.809867+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:40:39,348[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
[[34m2021-11-23 22:40:41,047[0m] {[34mdagbag.py:[0m305} ERROR[0m - Failed to import: /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
Traceback (most recent call last):
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/models/dagbag.py", line 302, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/airflow/airflow/dags/_call_process_vendor_inventory.py", line 30, in <module>
    DELETE_PREV_LAND_RECS="DELETE FROM AIRFLOW_POC.LAND_SALES_OPS_INVENTORY_" + file_type + " WHERE VENDOR_ID = '" + vendor_num + "';"
NameError: name 'file_type' is not defined
[[34m2021-11-23 22:40:41,085[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:40:41,085[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Traceback (most recent call last):
  File "/home/airflow/.venv/airflow/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/__main__.py", line 40, in main
    args.func(args)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/cli.py", line 89, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/cli/commands/task_command.py", line 217, in task_run
    dag = get_dag(args.subdir, args.dag_id)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/cli.py", line 189, in get_dag
    'parse.'.format(dag_id)
airflow.exceptions.AirflowException: dag_id could not be found: call_process_vendor_inventory. Either the dag did not exist or it failed to parse.
[[34m2021-11-23 22:40:41,526[0m] {[34msequential_executor.py:[0m66} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T22:40:37.809867+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py']' returned non-zero exit status 1..[0m
[[34m2021-11-23 22:40:41,526[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T22:40:37.809867+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:40:42,704[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
[[34m2021-11-23 22:40:46,173[0m] {[34mdagbag.py:[0m305} ERROR[0m - Failed to import: /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
Traceback (most recent call last):
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/models/dagbag.py", line 302, in _load_modules_from_file
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/airflow/airflow/dags/_call_process_vendor_inventory.py", line 30, in <module>
    DELETE_PREV_LAND_RECS="DELETE FROM AIRFLOW_POC.LAND_SALES_OPS_INVENTORY_" + file_type + " WHERE VENDOR_ID = '" + vendor_num + "';"
NameError: name 'file_type' is not defined
[[34m2021-11-23 22:40:46,209[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:40:46,210[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Traceback (most recent call last):
  File "/home/airflow/.venv/airflow/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/__main__.py", line 40, in main
    args.func(args)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/cli.py", line 89, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/cli/commands/task_command.py", line 217, in task_run
    dag = get_dag(args.subdir, args.dag_id)
  File "/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/cli.py", line 189, in get_dag
    'parse.'.format(dag_id)
airflow.exceptions.AirflowException: dag_id could not be found: call_process_vendor_inventory. Either the dag did not exist or it failed to parse.
[[34m2021-11-23 22:40:46,715[0m] {[34msequential_executor.py:[0m66} ERROR[0m - Failed to execute task Command '['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T22:40:37.809867+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py']' returned non-zero exit status 1..[0m
[[34m2021-11-23 22:40:46,715[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Set_Variables execution_date=2021-11-23 22:40:37.809867+00:00 exited with status failed for try_number 1[0m
[[34m2021-11-23 22:40:46,716[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.File_Type execution_date=2021-11-23 22:40:37.809867+00:00 exited with status failed for try_number 1[0m
[[34m2021-11-23 22:40:46,728[0m] {[34mscheduler_job.py:[0m1235} ERROR[0m - Executor reports task instance <TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 22:40:37.809867+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-11-23 22:40:46,730[0m] {[34mscheduler_job.py:[0m1235} ERROR[0m - Executor reports task instance <TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 22:40:37.809867+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-11-23 22:41:32,007[0m] {[34mscheduler_job.py:[0m855} WARNING[0m - Set 2 task instances to state=None as their associated DagRun was not in RUNNING state[0m
[[34m2021-11-23 22:42:48,376[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 2 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 22:42:47.685237+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 22:42:47.685237+00:00 [scheduled]>[0m
[[34m2021-11-23 22:42:48,378[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 2 task instances ready to be queued[0m
[[34m2021-11-23 22:42:48,378[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:42:48,378[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 3/16 running and queued tasks[0m
[[34m2021-11-23 22:42:48,379[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 22:42:47.685237+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 22:42:47.685237+00:00 [scheduled]>[0m
[[34m2021-11-23 22:42:48,380[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Set_Variables', execution_date=datetime.datetime(2021, 11, 23, 22, 42, 47, 685237, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 22:42:48,380[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T22:42:47.685237+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:42:48,380[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='File_Type', execution_date=datetime.datetime(2021, 11, 23, 22, 42, 47, 685237, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 22:42:48,380[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T22:42:47.685237+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:42:48,382[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T22:42:47.685237+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:42:49,320[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:42:50,746[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:42:50,746[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23T22:42:47.685237+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:42:51,587[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T22:42:47.685237+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:42:52,440[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:42:54,000[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:42:54,000[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23T22:42:47.685237+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:42:54,800[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Set_Variables execution_date=2021-11-23 22:42:47.685237+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:42:54,800[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.File_Type execution_date=2021-11-23 22:42:47.685237+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:42:54,897[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 22:42:47.685237+00:00 [scheduled]>[0m
[[34m2021-11-23 22:42:54,899[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 22:42:54,899[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:42:54,900[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 22:42:47.685237+00:00 [scheduled]>[0m
[[34m2021-11-23 22:42:54,901[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Print_Variables', execution_date=datetime.datetime(2021, 11, 23, 22, 42, 47, 685237, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-11-23 22:42:54,901[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T22:42:47.685237+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:42:54,903[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T22:42:47.685237+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:42:55,656[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:42:57,810[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:42:57,811[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23T22:42:47.685237+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:42:58,847[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Print_Variables execution_date=2021-11-23 22:42:47.685237+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:42:58,921[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 22:42:47.685237+00:00 [scheduled]>[0m
[[34m2021-11-23 22:42:58,923[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 22:42:58,923[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:42:58,923[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 22:42:47.685237+00:00 [scheduled]>[0m
[[34m2021-11-23 22:42:58,925[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='snowflake_op_sql_str', execution_date=datetime.datetime(2021, 11, 23, 22, 42, 47, 685237, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-11-23 22:42:58,926[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T22:42:47.685237+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:42:58,927[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'snowflake_op_sql_str', '2021-11-23T22:42:47.685237+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:43:00,782[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:43:04,194[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:43:04,195[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23T22:42:47.685237+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:43:06,794[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.snowflake_op_sql_str execution_date=2021-11-23 22:42:47.685237+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:43:06,871[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.sample_file_test 2021-11-23 22:42:47.685237+00:00 [scheduled]>[0m
[[34m2021-11-23 22:43:06,873[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 22:43:06,873[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:43:06,873[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.sample_file_test 2021-11-23 22:42:47.685237+00:00 [scheduled]>[0m
[[34m2021-11-23 22:43:06,874[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='sample_file_test', execution_date=datetime.datetime(2021, 11, 23, 22, 42, 47, 685237, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-11-23 22:43:06,875[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sample_file_test', '2021-11-23T22:42:47.685237+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:43:06,876[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sample_file_test', '2021-11-23T22:42:47.685237+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:43:07,631[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:43:09,140[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:43:09,140[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.sample_file_test 2021-11-23T22:42:47.685237+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:43:18,056[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.sample_file_test execution_date=2021-11-23 22:42:47.685237+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:43:18,122[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun call_process_vendor_inventory @ 2021-11-23 22:42:47.685237+00:00: manual__2021-11-23T22:42:47.685237+00:00, externally triggered: True> successful[0m
[[34m2021-11-23 22:45:26,474[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 22:46:19,892[0m] {[34mdagrun.py:[0m645} WARNING[0m - Failed to get task '<TaskInstance: call_process_vendor_inventory.snowflake_op_sql_str 2021-11-23 21:21:08.463247+00:00 [None]>' for dag '<DAG: call_process_vendor_inventory>'. Marking it as removed.[0m
[[34m2021-11-23 22:46:19,893[0m] {[34mdagrun.py:[0m651} INFO[0m - Restoring task '<TaskInstance: call_process_vendor_inventory.sns_msg 2021-11-23 21:21:08.463247+00:00 [removed]>' which was previously removed from DAG '<DAG: call_process_vendor_inventory>'[0m
[[34m2021-11-23 22:48:21,731[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 2 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 22:48:20.861908+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 22:48:20.861908+00:00 [scheduled]>[0m
[[34m2021-11-23 22:48:21,733[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 2 task instances ready to be queued[0m
[[34m2021-11-23 22:48:21,733[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:48:21,733[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 3/16 running and queued tasks[0m
[[34m2021-11-23 22:48:21,733[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23 22:48:20.861908+00:00 [scheduled]>
	<TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23 22:48:20.861908+00:00 [scheduled]>[0m
[[34m2021-11-23 22:48:21,735[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Set_Variables', execution_date=datetime.datetime(2021, 11, 23, 22, 48, 20, 861908, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 22:48:21,735[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T22:48:20.861908+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:48:21,735[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='File_Type', execution_date=datetime.datetime(2021, 11, 23, 22, 48, 20, 861908, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-11-23 22:48:21,735[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T22:48:20.861908+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:48:21,737[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Set_Variables', '2021-11-23T22:48:20.861908+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:48:23,424[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:48:26,666[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:48:26,666[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Set_Variables 2021-11-23T22:48:20.861908+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:48:28,118[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'File_Type', '2021-11-23T22:48:20.861908+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:48:29,546[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:48:32,684[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:48:32,684[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.File_Type 2021-11-23T22:48:20.861908+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:48:33,623[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Set_Variables execution_date=2021-11-23 22:48:20.861908+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:48:33,623[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.File_Type execution_date=2021-11-23 22:48:20.861908+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:48:33,707[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 22:48:20.861908+00:00 [scheduled]>[0m
[[34m2021-11-23 22:48:33,709[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 22:48:33,709[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:48:33,709[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23 22:48:20.861908+00:00 [scheduled]>[0m
[[34m2021-11-23 22:48:33,711[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='Print_Variables', execution_date=datetime.datetime(2021, 11, 23, 22, 48, 20, 861908, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 3 and queue default[0m
[[34m2021-11-23 22:48:33,711[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T22:48:20.861908+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:48:33,712[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'Print_Variables', '2021-11-23T22:48:20.861908+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:48:34,503[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:48:36,013[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:48:36,013[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.Print_Variables 2021-11-23T22:48:20.861908+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:48:36,765[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.Print_Variables execution_date=2021-11-23 22:48:20.861908+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:48:36,825[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.sample_file_test 2021-11-23 22:48:20.861908+00:00 [scheduled]>[0m
[[34m2021-11-23 22:48:36,827[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 22:48:36,827[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:48:36,827[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.sample_file_test 2021-11-23 22:48:20.861908+00:00 [scheduled]>[0m
[[34m2021-11-23 22:48:36,829[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='sample_file_test', execution_date=datetime.datetime(2021, 11, 23, 22, 48, 20, 861908, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-11-23 22:48:36,829[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sample_file_test', '2021-11-23T22:48:20.861908+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:48:36,831[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sample_file_test', '2021-11-23T22:48:20.861908+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:48:37,602[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:48:39,060[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:48:39,060[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.sample_file_test 2021-11-23T22:48:20.861908+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:50:12,817[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.sample_file_test execution_date=2021-11-23 22:48:20.861908+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:50:13,009[0m] {[34mscheduler_job.py:[0m941} INFO[0m - 1 tasks up for execution:
	<TaskInstance: call_process_vendor_inventory.sns_msg 2021-11-23 22:48:20.861908+00:00 [scheduled]>[0m
[[34m2021-11-23 22:50:13,013[0m] {[34mscheduler_job.py:[0m975} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 126 open slots and 1 task instances ready to be queued[0m
[[34m2021-11-23 22:50:13,014[0m] {[34mscheduler_job.py:[0m1002} INFO[0m - DAG call_process_vendor_inventory has 2/16 running and queued tasks[0m
[[34m2021-11-23 22:50:13,014[0m] {[34mscheduler_job.py:[0m1063} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: call_process_vendor_inventory.sns_msg 2021-11-23 22:48:20.861908+00:00 [scheduled]>[0m
[[34m2021-11-23 22:50:13,018[0m] {[34mscheduler_job.py:[0m1105} INFO[0m - Sending TaskInstanceKey(dag_id='call_process_vendor_inventory', task_id='sns_msg', execution_date=datetime.datetime(2021, 11, 23, 22, 48, 20, 861908, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-11-23 22:50:13,018[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sns_msg', '2021-11-23T22:48:20.861908+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:50:13,027[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'call_process_vendor_inventory', 'sns_msg', '2021-11-23T22:48:20.861908+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/_call_process_vendor_inventory.py'][0m
[[34m2021-11-23 22:50:14,425[0m] {[34mdagbag.py:[0m448} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/_call_process_vendor_inventory.py[0m
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/snowflake/connector/options.py:78 UserWarning: You have an incompatible version of 'pyarrow' installed (3.0.0), please install a version that adheres to: 'pyarrow<0.18.0,>=0.17.0; extra == "pandas"'
/home/airflow/.venv/airflow/lib64/python3.7/site-packages/airflow/utils/decorators.py:94 DeprecationWarning: provide_context is deprecated as of 2.0 and is no longer required
[[34m2021-11-23 22:50:17,868[0m] {[34mexample_kubernetes_executor_config.py:[0m174} WARNING[0m - Could not import DAGs in example_kubernetes_executor_config.py: No module named 'kubernetes'[0m
[[34m2021-11-23 22:50:17,869[0m] {[34mexample_kubernetes_executor_config.py:[0m175} WARNING[0m - Install kubernetes dependencies with: pip install apache-airflow['cncf.kubernetes'][0m
Running <TaskInstance: call_process_vendor_inventory.sns_msg 2021-11-23T22:48:20.861908+00:00 [queued]> on host ip-10-192-10-230.ec2.internal
[[34m2021-11-23 22:50:19,592[0m] {[34mscheduler_job.py:[0m1206} INFO[0m - Executor reports execution of call_process_vendor_inventory.sns_msg execution_date=2021-11-23 22:48:20.861908+00:00 exited with status success for try_number 1[0m
[[34m2021-11-23 22:50:19,692[0m] {[34mdagrun.py:[0m445} INFO[0m - Marking run <DagRun call_process_vendor_inventory @ 2021-11-23 22:48:20.861908+00:00: manual__2021-11-23T22:48:20.861908+00:00, externally triggered: True> successful[0m
[[34m2021-11-23 22:50:26,540[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 22:55:26,596[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 23:00:26,647[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 23:05:26,668[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 23:10:26,747[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 23:15:26,799[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 23:20:26,864[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 23:25:26,919[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 23:30:26,979[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 23:35:27,060[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 23:40:27,144[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 23:45:27,232[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 23:50:27,289[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-23 23:55:27,339[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 00:00:27,389[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 00:05:27,447[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 00:10:27,503[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 00:15:27,582[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 00:20:27,653[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 00:25:27,703[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 00:30:27,719[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 00:35:27,773[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 00:40:27,837[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 00:45:27,886[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 00:50:27,935[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 00:55:28,031[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 01:00:28,090[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 01:05:28,140[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 01:10:28,196[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 01:15:28,252[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 01:20:28,300[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 01:25:28,357[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 01:30:28,429[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 01:35:28,538[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 01:40:28,588[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 01:45:28,649[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 01:50:28,704[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 01:55:28,758[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 02:00:28,825[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 02:05:28,878[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 02:10:28,904[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 02:15:28,957[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 02:20:29,023[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 02:25:29,077[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 02:30:29,136[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 02:35:29,185[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 02:40:29,239[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 02:45:29,299[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 02:50:29,361[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 02:55:29,539[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 03:00:29,672[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 03:05:29,740[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 03:10:29,793[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 03:15:29,874[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 03:20:29,938[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 03:25:29,992[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 03:30:30,025[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 03:35:30,105[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 03:40:30,165[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 03:45:30,217[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 03:50:30,285[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 03:55:30,338[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 04:00:30,387[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 04:05:30,441[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 04:10:30,494[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 04:15:30,560[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 04:20:30,620[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 04:25:30,672[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 04:30:30,746[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 04:35:30,814[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 04:40:30,891[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 04:45:30,975[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 04:50:31,045[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 04:55:31,100[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 05:00:31,262[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 05:05:31,337[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 05:10:31,386[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 05:15:31,437[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 05:20:31,464[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 05:25:31,537[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 05:30:31,598[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 05:35:31,669[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 05:40:31,732[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 05:45:31,781[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 05:50:31,837[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 05:55:31,889[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 06:00:31,944[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 06:05:32,005[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 06:10:32,116[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 06:15:32,172[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 06:20:32,246[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 06:25:32,305[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 06:30:32,416[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 06:35:32,491[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 06:40:32,551[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 06:45:32,619[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 06:50:32,679[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 06:55:32,700[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 07:00:32,755[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 07:05:32,829[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 07:10:32,890[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 07:15:32,986[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 07:20:33,042[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 07:25:33,112[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 07:30:33,172[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 07:35:33,221[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 07:40:33,325[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 07:45:33,419[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 07:50:33,480[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 07:55:33,543[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 08:00:33,608[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 08:05:33,683[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 08:10:33,738[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 08:15:33,787[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 08:20:33,838[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 08:25:33,892[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 08:30:33,936[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 08:35:34,023[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 08:40:34,079[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 08:45:34,142[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 08:50:34,212[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 08:55:34,261[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 09:00:34,314[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 09:05:34,396[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 09:10:34,448[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 09:15:34,512[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 09:20:34,577[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 09:25:34,638[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 09:30:34,691[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 09:35:34,784[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 09:40:34,871[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 09:45:34,904[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 09:50:34,990[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 09:55:35,096[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 10:00:35,160[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 10:05:35,212[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 10:10:35,257[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 10:15:35,320[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 10:20:35,388[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 10:25:35,442[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 10:30:35,554[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 10:35:35,639[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 10:40:35,690[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 10:45:35,742[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 10:50:35,811[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 10:55:35,864[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 11:00:35,918[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 11:05:35,968[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 11:10:36,058[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 11:15:36,115[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 11:20:36,180[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 11:25:36,234[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 11:30:36,285[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 11:35:36,341[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 11:40:36,389[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 11:45:36,452[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 11:50:36,491[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 11:55:36,541[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 12:00:36,598[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 12:05:36,652[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 12:10:36,711[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 12:15:36,771[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 12:20:36,837[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 12:25:36,863[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 12:30:36,913[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 12:35:36,994[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 12:40:37,054[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 12:45:37,103[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 12:50:37,165[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 12:55:37,219[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 13:00:37,277[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 13:05:37,358[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 13:10:37,421[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 13:15:37,475[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 13:20:37,526[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 13:25:37,606[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 13:30:37,666[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 13:35:37,716[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 13:40:37,779[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 13:45:37,830[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 13:50:37,882[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 13:55:37,934[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 14:00:38,024[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 14:05:38,084[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 14:10:38,174[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 14:15:38,230[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 14:20:38,288[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 14:25:38,349[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 14:30:38,405[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 14:35:38,500[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 14:40:38,563[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 14:45:38,651[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 14:50:38,702[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 14:55:38,758[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 15:00:38,823[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 15:05:38,874[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 15:10:38,927[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 15:15:39,035[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 15:20:39,134[0m] {[34mscheduler_job.py:[0m1834} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-11-24 15:23:37,633[0m] {[34mscheduler_job.py:[0m746} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[[34m2021-11-24 15:23:37,980[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 25780[0m
[[34m2021-11-24 15:23:38,011[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending Signals.SIGTERM to GPID 25780[0m
[[34m2021-11-24 15:23:38,013[0m] {[34mscheduler_job.py:[0m1301} INFO[0m - Exited execute loop[0m
